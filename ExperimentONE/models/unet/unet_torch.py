# -*- coding: utf-8 -*-
"""DeepLabv3plus_normalized0to1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qCHgOV7M-0_gzwE4RPJ2NC361KbiG5j
"""

# -------- Library --------
# Machine Learning

import torch
import torch.nn as nn
from torch import Tensor
import segmentation_models_pytorch as smp

# Data

from torchvision.datasets.vision import VisionDataset
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from torchvision import transforms

# Graph
from typing import Any, Callable, Optional
from tqdm import tqdm
import matplotlib.pyplot as plt

# Directory manager
import os
import glob
from copy import deepcopy
from pathlib import Path
import copy
import time

# Metrics

import argparse

from dataset import MyDataset
from utils import *

# -------- Set up --------

# Constants
IMG_HEIGHT = 512
IMG_WIDTH = 512

POTSDAM_DATA_PATH = '/home/pedro/Desktop/experimentosMateus/Projeto/Potsdam_split/train/'

# On NVIDIA architecture
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

# On Apple M chip architecture
# device = torch.device("mps")
print('Using ' + str(device) + ' GPU')


# DeepLabv3plus
def unet_model(num_channel: int, num_classes: int):
    model_unet = smp.Unet(encoder_name='resnet50', encoder_weights=None,
                          in_channels=num_channel, classes=num_classes, activation='softmax')

    return model_unet


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_type', required=True, default="UNet",
                        help="declare the model type to use, currently only support input being UNet")
    parser.add_argument("--random_seed", required=True, help="random seed")
    parser.add_argument("--batch_size", required=True, help="batch size")
    parser.add_argument("--dataset", required=True, default="Potsdam", help="Potsdam")
    parser.add_argument("--num_channel", required=True, default=3, help="number of Channels")
    parser.add_argument("--num_classes", required=True, default=6, help="number of classes")
    parser.add_argument("--num_epoch", required=True, help='number of epoches')
    parser.add_argument("--lr", required=True, help="learning rate")
    parser.add_argument("--model_path", default=None, help="the path to the pretrained model")

    args = parser.parse_args()

    model_type = args.model_type
    dataset = args.dataset
    num_channel = int(args.num_channel)
    num_classes = int(args.num_classes)
    batch_size = int(args.batch_size)
    random_seed = int(args.random_seed)
    num_epoch = int(args.num_epoch)
    base_lr = float(args.lr)

    now = datetime.now()
    create_dir('./log')
    logging.basicConfig(filename='./log/log_{}_{}_{}.txt'.format(model_type, dataset, str(now)), level=logging.INFO,
                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')
    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))
    logging.info("Batch size : {} , epoch num: {}, num_channel: {}, base_lr : {}, dataset: {}".format(batch_size,
                                                                                                      num_epoch,
                                                                                                      num_channel,
                                                                                                      base_lr, dataset))

    data_path = POTSDAM_DATA_PATH
    total_data = MyDataset(dir_path=data_path)
    train_set_size = int(len(total_data) * 0.8)
    test_set_size = len(total_data) - train_set_size

    train_set, test_set = data.random_split(total_data, [train_set_size, test_set_size],
                                            generator=torch.Generator().manual_seed(random_seed))
    logging.info("train size {} test size {}".format(train_set_size, test_set_size))

    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
    testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)

    dataloaders = {"train": trainloader, "test": testloader}
    dataset_sizes = {"train": len(trainloader), "test": len(testloader)}
    logging.info("size train : {}, size test {} ".format(dataset_sizes["train"], dataset_sizes["test"]))

    test_loss = []
    train_loss = []

    model = unet_model(num_channel=num_channel, num_classes=num_classes)
    best_model_wts = deepcopy(model.state_dict())

    # Specify the loss function
    # criterion = torch.nn.CrossEntropyLoss()
    diceloss = DiceLoss(num_classes)

    optimizer = optim.Adam(model.parameters(), lr=base_lr)

    best_loss = 100
    best_epoch = 0

    model.to(device)

    for epoch in range(num_epoch):
        # early stop, if the loss does not decrease for 50 epochs
        print('====== Epoch', epoch)
        if epoch > best_epoch + 50:
            break
        for phase in ['train', 'test']:
            running_loss = 0
            running_loss_wo_dis = 0
            running_loss_seg = 0
            s = time.time()  # start time for this epoch
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()

            for i, d in enumerate(dataloaders[phase]):

                img, semantic_seg_mask = d

                img = img.float()
                img = img.to(device)
                semantic_seg_mask = semantic_seg_mask.to(device)

                output = model(img)

                print('Pred', np.unique(torch.argmax(output, dim=1).data.cpu().numpy().ravel()))
                print('True', np.unique(semantic_seg_mask.data.cpu().numpy().ravel()))

                # CEloss = criterion(output, semantic_seg_mask)
                dice_loss = 0.4 * diceloss(output, semantic_seg_mask.float(), softmax=True)

                running_loss += dice_loss.item()

                if phase == 'train':
                    optimizer.zero_grad()
                    dice_loss.backward()
                    optimizer.step()
            e = time.time()
            epoch_loss = running_loss / dataset_sizes[phase]
            logging.info('Epoch {},: loss seg {}, {},time {}'.format(epoch + 1, epoch_loss, phase, e - s))

            if phase == 'train':
                train_loss.append(epoch_loss)
            else:
                test_loss.append(epoch_loss)

            if phase == 'test' and dice_loss < best_loss:
                best_loss = dice_loss
                best_epoch = epoch + 1
                best_model_wts = copy.deepcopy(model.state_dict())
                logging.info("Best val loss {} save at epoch {}".format(best_loss, epoch + 1))

        #draw_loss(train_loss, test_loss, str(now))

        create_dir('./saved')
        print('Salvando o modelo')
        torch.save(best_model_wts,
                   './saved/modelDeepLab_epoch:{}_testloss:{}_{}.pt'.format(best_epoch, best_loss, str(now)))
        logging.info(
            'Model saved. at {}'.format(
                './saved/modelDeepLab_epoch:{}_testloss:{}_{}.pt'.format(best_epoch, best_loss, str(now))))

        torch.cuda.empty_cache()

        model.load_state_dict(best_model_wts)
        model.eval()

        dice_acc_test = 0
        dice_loss_test = DiceLoss(num_classes)

        with torch.no_grad():
            print('Testing...')
            for i, d in enumerate(testloader, 0):
                img, semantic_seg_mask = d
                img = img.float()
                img = img.to(device)

                semantic_seg_mask = semantic_seg_mask.to(device)

                output1 = model(img)

                print('Pred', np.unique(torch.argmax(output1, dim=1).data.cpu().numpy().ravel()))
                print('True', np.unique(semantic_seg_mask.data.cpu().numpy().ravel()))

                d_l = dice_loss_test(output1, semantic_seg_mask.float(), softmax=True)
                dice_acc_test += 1 - d_l.item()

                #create_dir('./outputs')
                #cv2.imwrite('./outputs/seg_' + str(i), output1)

        logging.info("dice_acc {}".format(dice_acc_test / dataset_sizes['test']))


# ------------- Applying -------------

# Training CBERS4A RGB
if __name__ == '__main__':
    main()
    print('Pronto')
